ğŸ“Š Projeto Despesas - Pipeline ANS
Este Ã© um sistema de Web Scraping desenvolvido em Python para automatizar a coleta, processamento e visualizaÃ§Ã£o de dados de despesas (DemonstraÃ§Ãµes ContÃ¡beis/DC) da AgÃªncia Nacional de SaÃºde Suplementar (ANS).

[!IMPORTANT]

Nota do Desenvolvedor: Por conta do tempo, o projeto foi focado na estabilidade do ciclo principal (Coleta -> Processamento -> ValidaÃ§Ã£o). Ele serve como uma base sÃ³lida para aprendizado de processos ETL e automaÃ§Ã£o.

ğŸ”— Fonte de Dados
O projeto consome dados da API/FTP pÃºblica da ANS:

https://dadosabertos.ans.gov.br/FTP/PDA/

ğŸš€ Como Executar
Para rodar o pipeline completo, execute o arquivo principal a partir da raiz do projeto:

Bash
python src/app/main.py
Exemplo de Uso Interno
O fluxo foi desenhado para ser executado em sequÃªncia lÃ³gica:

Python
# 1. DOWNLOAD (Crawler)
crawler = ANSCrawler()
crawler.get_last_3_quarters(2023)

# 2. PROCESSAMENTO (Limpeza e Filtro ContÃ¡bil)
processor = DataProcessor("2023")
processor.unzip_all()
processor.consolidate_quarters()

# 3. VALIDAÃ‡ÃƒO (Enriquecimento e EstatÃ­sticas)
validator = ANSValidation("2023")
validator.generate_aggregate_expenses_and_statistics()
ğŸ“ Estrutura de Dados (Fluxo ETL)
O projeto organiza os dados em camadas para garantir a integridade:

Plaintext
data/
â”œâ”€â”€ raw/            # Arquivos .zip originais (Download bruto)
â”œâ”€â”€ extracted/      # CSVs descompactados (Processamento temporÃ¡rio)
â””â”€â”€ consolidated/   # Destino Final (Dados prontos para anÃ¡lise)
    â””â”€â”€ {ano}/
        â”œâ”€â”€ consolidado_despesas.zip   # Dados filtrados por regra contÃ¡bil
        â”œâ”€â”€ despesas_agregadas.csv     # Dados enriquecidos com CNPJ/RazÃ£o Social
        â””â”€â”€ estatisticas_despesas.csv  # Insights: MÃ©dia, Soma e Desvio PadrÃ£o
âš™ï¸ ConfiguraÃ§Ãµes (Settings)
Centralizadas em src/app/core/configs.py, utilizam caminhos dinÃ¢micos e Regex para maior adaptabilidade.

GestÃ£o de DiretÃ³rios: AutomaÃ§Ã£o na criaÃ§Ã£o e mapeamento de pastas.

ParÃ¢metros de Busca: Regex flexÃ­veis para identificar anos (YYYY/) e diferentes nomenclaturas de trimestres (ex: 1T2023, 2023_1_trimestre).

Controle de Ambiente: Flag ENV = "dev" para alternar comportamentos de teste.

ğŸ› ï¸ Funcionalidades e Camadas
1. Crawler Automatizado (Services/Crawler)
Gerencia o ciclo de vida da extraÃ§Ã£o com as seguintes caracterÃ­sticas:

Mapeamento Inteligente: Localiza seÃ§Ãµes de "DemonstraÃ§Ãµes ContÃ¡beis" e "Operadoras Ativas".

Download Resiliente: Usa streams para arquivos grandes e valida se o conteÃºdo Ã© binÃ¡rio (evita salvar erros HTML).

2. Processor (Services/Processor)
Realiza a "mÃ¡gica" dos dados usando Pandas com chunksize, processando arquivos pesados sem estourar a memÃ³ria RAM.

Filtro ContÃ¡bil: * Contas iniciadas em 411 (Eventos/Sinistros).

DescriÃ§Ãµes que contenham "despesa" e ("evento" ou "sinistro").

ConsolidaÃ§Ã£o: Agrupa por Operadora/Ano/Trimestre com soma de valores.

3. Enrichment & Validation (Services/Validation)
O "Check-mate" dos dados. Cruza as despesas com o cadastro de operadoras (active_operators.csv).

Merge: Left join utilizando o REG_ANS como chave primÃ¡ria.

EstatÃ­sticas: Gera automaticamente Soma, MÃ©dia e Desvio PadrÃ£o por RazÃ£o Social e UF.

Integridade: Filtra apenas operadoras com CNPJ vÃ¡lido.

ğŸ“Š Desafio Adicional (Resultados)
Ao final da execuÃ§Ã£o, o arquivo estatisticas_despesas.csv entrega uma tabela resumida com mÃ©tricas de variaÃ§Ã£o e performance financeira por estado e operadora.

Desenvolvido por [devlimax] ğŸš€

AbraÃ§os e obrigado pela atenÃ§Ã£o!
