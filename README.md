# Projeto Despesas 

Este √© um sistema de **Web Scraping** desenvolvido em python para automatizar a coleta, processamento e visualiza√ß√£o de dados de despesas (Demonstra√ß√µs Contabeis/DC).
O projeto navega pela API Publica disponibilizada no Desafio.
## URL -> ("https://dadosabertos.ans.gov.br/FTP/PDA/").

**OBS**: por conta do tempo, n√£o consegui finalizar o projeto por completo, e nem documenta-lo 100%, apenas registrei as etapas e processos mais importantes para o ciclo ser iniciado e finalizado sem erros, mas ainda sim irei usar esse projeto como base, para um melhor aprendizado de processos.
desde ja agrede√ßo o tempo e aten√ßao dedicados a visualiza√ß√£o da minha solu√ß√£o. Abra√ßos!!

# Execu√ß√£o do projeto:
    Para executar o projeto por completo sem nenhum tipo de erro, pe√ßo que por favor, inicie o arquivo na **main.py** localizado na pasta "src/app", rode o arquivo via terminal dentro do diretorio raiz do Projeto.

## üìÅ Estrutura de Pastas de Dados
O fluxo de dados segue este caminho dentro do projeto:

data/
‚îú‚îÄ‚îÄ raw/          # Arquivos .zip originais baixados pelo crawler
‚îú‚îÄ‚îÄ extracted/    # CSVs originais ap√≥s descompacta√ß√£o
‚îî‚îÄ‚îÄ consolidated/ # Destino Final
    ‚îî‚îÄ‚îÄ {ano}/
        ‚îú‚îÄ‚îÄ consolidado_despesas.zip   # Sa√≠da do processamento
        ‚îú‚îÄ‚îÄ despesas_agregadas.csv     # Dados enriquecidos (Enriquecimento)
        ‚îî‚îÄ‚îÄ estatisticas_despesas.csv  # Insights estat√≠sticos (Valida√ß√£o)
------------------------------------------------------------------------------------------------------------------------------------------------------------------

# ‚öôÔ∏è Configura√ß√µes Centrais (Settings):
    O cora√ß√£o do projeto reside no arquivo core/configs.py. Ele utiliza caminhos din√¢micos e padr√µes de busca (Regex) para garantir que o sistema se adapte a diferentes anos e estruturas de pastas no servidor da ANS.

    üìÇ Gest√£o de Diret√≥rios
    O projeto segue uma estrutura de camadas para garantir a integridade dos dados:
        - PATH_DIR: Pasta raiz de dados (data/).

        - OUTPUT_DIR_RAW: Onde os arquivos brutos (Zips) s√£o armazenados logo ap√≥s o download.

        - OUTPUT_DIR_EXTRACTED: Pasta tempor√°ria para descompacta√ß√£o e leitura dos CSVs.

        - OUTPUT_DIR_CONSOLIDATED: Local final onde os relat√≥rios enriquecidos e estat√≠sticas s√£o gerados.

    üåê Par√¢metros de Conex√£o e Busca:
        - BASE_URL: Endpoint oficial da ANS para dados abertos.

        - FILTER_PAGE_QUARTERS: Par√¢metros de ordena√ß√£o para garantir a captura dos trimestres mais recentes primeiro. 

        - REGEX_PATTERN_YEAR: Identifica pastas no formato YYYY/ (Ex: 2023/).

        - REGEX_PATTERN_QUARTER: Identifica arquivos de trimestres em m√∫ltiplos formatos (Ex: 2023_1_trimestre.zip, 1T2023.zip).

    üöÄ Como Customizar o Ambiente
        O projeto possui uma flag de ambiente para controle de comportamento:

        Python
        # No arquivo core/configs.py
        ENV: str = "dev"
# ------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Funcionalidades -> Services/

**Crawler Automatizado** -> Navega por links para identificar arquivos de despesas e o relatorio de operadoras de sa√∫de ativas.

A classe principal ANSCrawler no arquivo src/app/services/crawler.py gerencia o ciclo de vida da extra√ß√£o:

Mapeamento de Origem: Utiliza URLs base configuradas via Settings para localizar as se√ß√µes de "Demonstra√ß√µes Cont√°beis" e "Operadoras Ativas".
Filtro Temporal: Localiza pastas espec√≠ficas por ano utilizando Express√µes Regulares (Regex).
Coleta de Trimestres: Acessa as p√°ginas internas e identifica os √∫ltimos 3 arquivos .zip dispon√≠veis para download.
Download Resiliente: Realiza o download via stream (para lidar com arquivos grandes) e valida se o conte√∫do recebido √© bin√°rio, evitando salvar p√°ginas de erro HTML como se fossem dados.

## Descri√ß√£o dos M√©todos Principais:

M√©todo -> **_get_page_quarters_by_year()**
Descri√ß√£o: M√©todo privado que busca o link da pasta do ano espec√≠fico no portal

M√©todo -> **get_last_3_quarters()**
Descri√ß√£o: Navega at√© a p√°gina de trimestres e retorna uma lista com os 3 links de download mais recentes

M√©todo -> **get_active_operators()**
Descri√ß√£o: Localiza e retorna a URL direta para o relat√≥rio CSV de operadoras ativas.

M√©todo -> **download_file()**
Descri√ß√£o: Gerencia a persist√™ncia no disco, criando diret√≥rios automaticamente e tratando erros de conex√£o.

## Estrutura de Sa√≠da
O sistema organiza os arquivos baixados seguindo a hierarquia definida no Settings.OUTPUT_DIR_RAW:

data/
‚îî‚îÄ‚îÄ raw/
    ‚îú‚îÄ‚îÄ {folder_name}/
    ‚îÇ   ‚îî‚îÄ‚îÄ {filename}.zip
    ‚îî‚îÄ‚îÄ operadoras_ativas.csv
------------------------------------------------------------------------------------------------------------------------------------------------------------------

**Processor**

A classe DataProcessor realiza a "m√°gica" dos dados, utilizando Pandas com processamento em blocos (chunksize) para n√£o estourar a mem√≥ria RAM ao ler arquivos CSV pesados que foram extraidos do processamento de download do **Crawler**.

**OBS**: se voce tentar utilizar o DataProcessor sem antes ter feito o download dos arquivos .zip por meio do crawler, ira resultar em erro e o algoritimo n√£o sera realizado de forma 100% efetiva.

## üîç L√≥gica de Filtragem Cont√°bil
Para garantir a precis√£o dos dados de despesas, o processador aplica os seguintes filtros:
    - C√≥digo Cont√°bil: Apenas contas que iniciam com 411 (Referentes a eventos/sinistros).
    - Descri√ß√£o: Filtra registros onde a descri√ß√£o cont√©m termos como "despesa" E ("evento" OU "sinistro").

## Descri√ß√£o dos M√©todos Principais:

M√©todo -> **unzip_all()**
Descri√ß√£o: Extrai todos os arquivos baixados para a pasta de processamento.

M√©todo -> **_get_consolidate_data()**
Descri√ß√£o: L√™ os CSVs em peda√ßos (chunks), aplica os filtros e mapeia cada linha para o objeto **ExpenseRecord**, ap√≥s isso cada objeto √© adicionado a lista privada da classe Principal. -> **self._consolidated = []**.

M√©todo -> **consolidate_quarters()**
Descri√ß√£o: Agrupa os dados por Operadora/Ano/Trimestre, soma os valores e gera o CSV final formatado em Real (R$).

## Estrutura de Sa√≠da
O sistema organiza os arquivos extraidos e consolidados seguindo a hierarquia definida no Settings.OUTPUT_DIR_EXTRACTED e Settings.OUTPUT_DIR_CONSOLIDATED:

data/    
‚îú‚îÄ‚îÄ extracted/
‚îÇ    ‚îú‚îÄ‚îÄ {folder_name}/
‚îÇ        ‚îî‚îÄ‚îÄ {filename}.csv
‚îú‚îÄ‚îÄ consolidated/
    ‚îú‚îÄ‚îÄ {folder_name}/
        ‚îî‚îÄ‚îÄ consolidado_despesas.zip
------------------------------------------------------------------------------------------------------------------------------------------------------------------

**Enrichment and Validation**

O arquivo **enrichment.py** √© respons√°vel pelo "Check-mate" dos dados. 
Ele utiliza o registro da operadora **(REG_ANS)** contido no **active_operators.csv** e **consolidado despesas.csv** como chave prim√°ria para enriquecer o relat√≥rio financeiro.

## üß© Funcionalidades Principais:
    - Merge de Dados: Realiza um left join entre as despesas consolidadas e o cadastro de operadoras ativas.
    - C√°lculo Estat√≠stico: Gera automaticamente a Soma Total, M√©dia Trimestral e o Desvio Padr√£o das despesas por Raz√£o Social e Estado (UF).
    - Valida√ß√£o de Integridade: Garante que apenas operadoras com CNPJ v√°lido e dados presentes no cruzamento de planilhas sejam reportadas.

    **Relat√≥rios Gerados:**
        - despesas_agregadas.csv -> Relat√≥rio detalhado com CNPJ, Raz√£o Social, UF, Modalidade e Valor.

        **Resposta Desafio Adicional:**
        - estatisticas_despesas.csv -> Tabela resumida com m√©tricas de m√©dia, soma e varia√ß√£o (desvio padr√£o).
# ------------------------------------------------------------------------------------------------------------------------------------------------------------------


# üöÄ Como Executar o Pipeline Completo
O projeto foi desenhado para ser executado em sequ√™ncia:

Python
    # 1. DOWNLOAD
    crawler = ANSCrawler()
    crawler.get_last_3_quarters(2023)

    # 2. PROCESSAMENTO (Limpeza e Filtro Cont√°bil)
    processor = DataProcessor("2023")
    processor.unzip_all()
    processor.consolidate_quarters()

    # 3. VALIDA√á√ÉO (Enriquecimento e Estat√≠sticas)
    validator = ANSValidation("2023")
    validator.generate_aggregate_expenses_and_statistics()

    
